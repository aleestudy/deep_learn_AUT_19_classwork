{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 1. Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# Other system packages\n",
    "import os\n",
    "\n",
    "# Keras functions \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, Flatten, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the AT1A data\n",
    "\n",
    "# Set the dataset directory\n",
    "dataset_dir = os.getcwd() + \"/data\"\n",
    "\n",
    "# Get the data labels\n",
    "labels_file = dataset_dir + \"/train_selected.csv\"\n",
    "data_labels = pd.read_csv(labels_file)\n",
    "\n",
    "data_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X files\n",
    "file_list = [dataset_dir + \"/\" + str(x) + \".png\" for x in list(data_labels[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "data_labels[\"class\"] = np.where(data_labels['label']=='automobile', 1, 0)\n",
    "data_labels[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarise_data(dataset):\n",
    "    \n",
    "    new_dataset = dataset/255.\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global X_train, X_test, y_train, y_test, X, y\n",
    "    \n",
    "    X = np.array([np.array(Image.open(fname)) for fname in file_list])\n",
    "    y = np.array(data_labels[\"class\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "#     y_train = y_train.reshape(1, y_train.shape[0]) # NOTE NO NEED FOR THIS WITH KERAS\n",
    "#     y_test = y_test.reshape(1, y_test.shape[0]) # NOTE NO NEED FOR THIS WITH KERAS\n",
    "    \n",
    "    \n",
    "    # Reshape the training and test examples \n",
    "#     X_train_f = X_train.reshape(X_train.shape[0], -1) # NOTE THE REMOVAL OF TRANSPOSE HERE\n",
    "#     X_test_f = X_test.reshape(X_test.shape[0], -1) # NOTE THE REMOVAL OF TRANSPOSE HERE\n",
    "    \n",
    "    # We could also use the keras flatten if we wanted\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\n",
    "    \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    X_train = standarise_data(X_train)\n",
    "    X_test = standarise_data(X_test)\n",
    "    \n",
    "\n",
    "    print (\"Flatten X_train: \" + str(X_train.shape))\n",
    "    print (\"Flatten X_test: \" + str(X_test.shape))\n",
    "    \n",
    "    print (\"y_train: \" + str(y_train.shape))\n",
    "    print (\"y_test: \" + str(y_test.shape))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "\n",
    "# Note we will save y_test for our sklearn metrics\n",
    "y_test_skl = y_test.copy()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)\n",
    "print (\"y_train: \" + str(y_train.shape))\n",
    "print (\"y_test: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.1 Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out with a simple CNN architecture. We will have:\n",
    "\n",
    "* Conv > Conv (Relu) 36 filters each time.\n",
    "* Max Pooling (size 2)\n",
    "* Fully connected layer (size 64)\n",
    "* Fully connected layer (size 2)\n",
    "    * Sigmoid activation to get us to binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\", input_shape=[32, 32, 3]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\")) #You could add activations inside this if you like, or as a separate layer\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Flatten()) # Stretching out for our FC layer\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\t\t\n",
    "# Binary classifier\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get our score\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get raw probabilities\n",
    "\n",
    "# Let's set a nice print option\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "predictions = model.predict_proba(X_test)\n",
    "predictions[0:5]\n",
    "\n",
    "# Here we can see it is probabilities of two classes. Negative first, then positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:,1].shape # This is how we can slice out the probability of positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:,1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get class predictions\n",
    "predictions_classes = model.predict_classes(X_test)\n",
    "predictions_classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some sklearn scores\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_skl), list(predictions_classes)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_skl, predictions_classes))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_skl, predictions_classes))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_skl, predictions[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.2 Deeper + batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go a bit deeper and add some batch normalisation to assist us with this depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some modifications to our network:\n",
    "\n",
    "* Triple the depth\n",
    "* Add Batch Norm after the convolutions, before activation\n",
    "* Give it some more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\", input_shape=[32, 32, 3]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(filters=36, kernel_size=3, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Flatten()) # Stretching out for our FC layer\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\t\t\n",
    "# Sigmoid classifier\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                   batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get our score\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our probabilities, classes and scores\n",
    "predictions = model.predict_proba(X_test)\n",
    "predictions_classes = model.predict_classes(X_test)\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_skl), list(predictions_classes)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_skl, predictions_classes))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_skl, predictions_classes))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_skl, predictions[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.3 Play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn, play around with adding more layers, try different optimisers or some better settings on the existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2.4 Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how a prebuilt model performs compared to our playing around above. You can find many built in networks on the keras documentation page:\n",
    "\n",
    "* http://keras.io/applications/\n",
    "\n",
    "Let's use Xception trained on imagenet and see how well it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation, some important inputs:\n",
    "\n",
    "`include_top`: whether to include the 3 fully-connected layers at the top of the network.\n",
    "\n",
    "`weights:` one of None (random initialization) or 'imagenet' (pre-training on ImageNet).\n",
    "\n",
    "`classes:` optional number of classes to classify images into, only to be specified if include_top is  True, and if no weights argument is specified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that these prebuilt architectures expect a certain input, so our images will need to be reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.4.1 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly let us take a smaller sample of images\n",
    "X_train = X_train[0:750]\n",
    "y_train = y_train[0:750]\n",
    "\n",
    "X_test = X_test[0:250]\n",
    "y_test = y_test[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly let us create an instance of the prebuilt model\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "\n",
    "InceptionV3_prebuilt = InceptionV3(weights='imagenet', include_top=False, input_shape=[299, 299, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving this out for use\n",
    "with open('inception_model.pickle', 'wb') as file_out:\n",
    "    pickle.dump(InceptionV3_prebuilt, file_out)\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip to here to read in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('inception_model.pickle', 'rb') as file_in:\n",
    "    InceptionV3_prebuilt = pickle.load(file_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reshaping the images you can use a variety of packages:\n",
    "\n",
    "* https://scikit-image.org/\n",
    "* opencv (https://stackoverflow.com/questions/48121916/numpy-resize-rescale-image)\n",
    "* You can also use keras built in image processing which also augments on the fly\n",
    "    * https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some reshaping\n",
    "import skimage.transform\n",
    "\n",
    "X_train_reshape = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for image in X_train:\n",
    "    \n",
    "    count +=1 \n",
    "    if count % 500 == 0:\n",
    "        print(\"Done {} images\".format(count))\n",
    "        \n",
    "    new_image = skimage.transform.resize(image, (299, 299), mode='constant')\n",
    "    X_train_reshape.append(new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some reshaping\n",
    "import skimage.transform\n",
    "\n",
    "X_test_reshape = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for image in X_test:\n",
    "    \n",
    "    count +=1 \n",
    "    if count % 500 == 0:\n",
    "        print(\"Done {} images\".format(count))\n",
    "        \n",
    "    new_image = skimage.transform.resize(image, (299, 299), mode='constant')\n",
    "    X_test_reshape.append(new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into numpy array\n",
    "X_train_reshape = np.array(X_train_reshape)\n",
    "\n",
    "X_test_reshape = np.array(X_test_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prove we have the right shapes now\n",
    "print(X_train.shape)\n",
    "print(X_train_reshape.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_test_reshape.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(getsizeof(X_train_reshape)/1000000)\n",
    "print(getsizeof(X_test_reshape)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving this out for use\n",
    "with open('X_train_inception.pickle', 'wb') as file_out:\n",
    "    pickle.dump(X_train_reshape, file_out)\n",
    "    file_out.close()\n",
    "\n",
    "with open('X_test_inception.pickle', 'wb') as file_out:\n",
    "    pickle.dump(X_test_reshape, file_out)\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip to here to read in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('X_train_inception.pickle', 'rb') as file_in:\n",
    "    X_train_reshape = pickle.load(file_in)\n",
    "    file_in.close()\n",
    "\n",
    "with open('X_train_inception.pickle', 'rb') as file_in:\n",
    "    X_test_reshape = pickle.load(file_in)\n",
    "    file_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.4.2 No retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the layers to not be trainable\n",
    "\n",
    "for layer in InceptionV3_prebuilt.layers[:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us add our dense final layer\n",
    "x = InceptionV3_prebuilt.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "preds = Dense(2, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "final_model = Model(input=InceptionV3_prebuilt.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers \n",
    "final_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = final_model.fit(X_train_reshape, y_train, epochs=5,\n",
    "                    validation_data=(X_test_reshape, y_test),\n",
    "                   batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.4.3 Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is over to you, try unfreezing some layers and retraining. Or adding on more layers to the end. See what you can come up with!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
